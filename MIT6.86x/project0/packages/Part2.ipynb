{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 6.86x - Introduction to ML Packages (Part 2)\n",
    "\n",
    "This tutorial is designed to provide a short introduction to deep learning with PyTorch.\n",
    "\n",
    "You can start studying this tutorial as you work through unit 3 of the course.\n",
    "\n",
    "For more resources, check out [the PyTorch tutorials](https://pytorch.org/tutorials/)! There are many more in-depth examples available there.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Source code for this notebook hosted at: https://github.com/varal7/ml-tutorial\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## PyTorch\n",
    "\n",
    "[PyTorch](https://pytorch.org) is a flexible scientific computing package targetted towards gradient-based deep learning. Its low-level API closely follows [NumPy](http://www.numpy.org/). However, there are a several key additions:\n",
    "\n",
    "- GPU support!\n",
    "- Automatic differentiation!\n",
    "- Deep learning modules!\n",
    "- Data loading!\n",
    "- And other generally useful goodies.\n",
    "\n",
    "If you don't have GPU enabled hardward, don't worry. Like NumPy, PyTorch runs pre-compiled, highly efficient C code to handle all intensive backend functions.\n",
    "\n",
    "Go to pytorch.org to download the correct package for your computing environment.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Start by importing torch\n",
    "import torch"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-02-20T16:35:28.485220Z",
     "start_time": "2019-02-20T16:35:28.476294Z"
    }
   },
   "source": [
    "### Tensors\n",
    "\n",
    "Tensors are PyTorch's equivalent of NumPy ndarrays.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-02-20T16:35:28.492012Z",
     "start_time": "2019-02-20T16:35:28.487024Z"
    }
   },
   "outputs": [],
   "source": [
    "# Construct a bunch of ones\n",
    "some_ones = torch.ones(2, 2)\n",
    "print(some_ones)\n",
    "\n",
    "# Construct a bunch of zeros\n",
    "some_zeros = torch.zeros(2, 2)\n",
    "print(some_zeros)\n",
    "\n",
    "# Construct some normally distributed values\n",
    "some_normals = torch.randn(2, 2)\n",
    "print(some_normals)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-02-20T16:35:28.497368Z",
     "start_time": "2019-02-20T16:35:28.494171Z"
    },
    "scrolled": true
   },
   "source": [
    "PyTorch tensors and NumPy ndarrays even share the same memory handles, so you can switch between the two types essentially for free:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "torch_tensor = torch.randn(5, 5)\n",
    "numpy_ndarray = torch_tensor.numpy()\n",
    "back_to_torch = torch.from_numpy(numpy_ndarray)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-02-20T16:35:28.564975Z",
     "start_time": "2019-02-20T16:35:28.499321Z"
    }
   },
   "source": [
    "Like NumPy, there are a zillion different operations you can do with tensors. Best thing to do is to go to https://pytorch.org/docs/stable/tensors.html if you know you want to do something to a tensor but don't know how!\n",
    "\n",
    "We can cover a few major ones here:\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-02-19T16:57:10.540510Z",
     "start_time": "2019-02-19T16:57:10.496709Z"
    }
   },
   "source": [
    "In the Numpy tutorial, we have covered the basics of Numpy, numpy arrays, element-wise operations, matrices operations and generating random matrices.\n",
    "In this section, we'll cover indexing, slicing and broadcasting, which are useful concepts that will be reused in `Pandas` and `PyTorch`.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create two tensors\n",
    "a = torch.randn(5, 5)\n",
    "b = torch.randn(5, 5)\n",
    "print(a)\n",
    "print(b)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Indexing by i,j\n",
    "another_tensor = a[2, 2]\n",
    "print(another_tensor)\n",
    "\n",
    "# The above returns a tensor type! To get the python value:\n",
    "python_value = a[2, 2].item()\n",
    "print(python_value)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Getting a whole row or column or range\n",
    "first_row = a[0, :]\n",
    "first_column = a[:, 0]\n",
    "combo = a[2:4, 2:4]\n",
    "print(combo)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Addition\n",
    "c = a + b\n",
    "\n",
    "# Elementwise multiplication: c_ij = a_ij * b_ij\n",
    "c = a * b\n",
    "\n",
    "# Matrix multiplication: c_ik = a_ij * b_jk\n",
    "c = a.mm(b)\n",
    "\n",
    "# Matrix vector multiplication\n",
    "c = a.matmul(b[:, 0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "a = torch.randn(5, 5)\n",
    "print(a.size())\n",
    "\n",
    "vec = a[:, 0]\n",
    "print(vec.size())\n",
    "\n",
    "# Matrix multiple 5x5 * 5x5 --> 5x5\n",
    "aa = a.mm(a)\n",
    "\n",
    "# matrix vector 5x5 * 5 --> 5\n",
    "v1 = a.matmul(vec)\n",
    "print(v1)\n",
    "\n",
    "\n",
    "vec_as_matrix = vec.view(5, 1)\n",
    "v2 = a.mm(vec_as_matrix)\n",
    "print(v2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In-place operations exist to, generally denoted by a trailing '_' (e.g. my_tensor.my_inplace_function_).\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Add one to all elements\n",
    "a.add_(1)\n",
    "\n",
    "# Divide all elements by 2\n",
    "a.div_(2)\n",
    "\n",
    "# Set all elements to 0\n",
    "a.zero_()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Manipulate dimensions...\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Add a dummy dimension, e.g. (n, m) --> (n, m, 1)\n",
    "a = torch.randn(10, 10)\n",
    "\n",
    "# At the end\n",
    "print(a.unsqueeze(-1).size())\n",
    "\n",
    "# At the beginning\n",
    "print(a.unsqueeze(0).size())\n",
    "\n",
    "# In the middle\n",
    "print(a.unsqueeze(1).size())\n",
    "\n",
    "# What you give you can take away\n",
    "print(a.unsqueeze(0).squeeze(0).size())\n",
    "\n",
    "# View things differently, i.e. flat\n",
    "print(a.view(100, 1).size())\n",
    "\n",
    "# Or not flat\n",
    "print(a.view(50, 2).size())\n",
    "\n",
    "# Copy data across a new dummy dimension!\n",
    "a = torch.randn(2)\n",
    "a = a.unsqueeze(-1)\n",
    "print(a)\n",
    "print(a.expand(2, 3))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If you have a GPU...\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check if you have it\n",
    "do_i_have_cuda = torch.cuda.is_available()\n",
    "\n",
    "if do_i_have_cuda:\n",
    "    print(\"Using fancy GPUs\")\n",
    "    # One way\n",
    "    a = a.cuda()\n",
    "    a = a.cpu()\n",
    "\n",
    "    # Another way\n",
    "    device = torch.device(\"cuda\")\n",
    "    a = a.to(device)\n",
    "\n",
    "    device = torch.device(\"cpu\")\n",
    "    a = a.to(device)\n",
    "else:\n",
    "    print(\"CPU it is!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "And many more!\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## A Quick Note about Batching\n",
    "\n",
    "In most ML applications we do mini-batch stochastic gradient descent instead of pure stochastic gradient descent.\n",
    "\n",
    "Mini-batch SGD is a step between full gradient descent and stochastic gradient descent by computing the average gradient over a small number of examples.\n",
    "\n",
    "In a nutshell, given `n` examples:\n",
    "\n",
    "- **Full GD:** dL/dw = average over all `n` examples. One step per `n` examples.\n",
    "- **SGD:** dL/dw = point estimate over a single example. `n` steps per `n` examples.\n",
    "- **Mini-batch SGD:** dL/dw = average over `m << n` examples. `n / m` steps per `n` examples.\n",
    "\n",
    "Advantages of mini-batch SGD include a more stable gradient estimate and computational efficiency on modern hardware (exploiting parallelism gives sub-linear to constant time complexity, especially on GPU).\n",
    "\n",
    "In PyTorch, batched tensors are represented as just another dimension. Most of the deep learning modules assume batched tensors as input (even if the batch size is just 1).\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Batched matrix multiply\n",
    "a = torch.randn(10, 5, 5)\n",
    "b = torch.randn(10, 5, 5)\n",
    "\n",
    "# The same as for i in 1 ... 10, c_i = a[i].mm(b[i])\n",
    "c = a.bmm(b)\n",
    "\n",
    "print(c.size())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-02-20T16:35:28.575416Z",
     "start_time": "2019-02-20T16:35:28.571576Z"
    }
   },
   "source": [
    "## Autograd: Automatic Differentiation!\n",
    "\n",
    "Along with the flexible deep learning modules (to follow) this is the best part of using a package PyTorch.\n",
    "\n",
    "What is autograd? It _automatically_ computes _gradients_. All those complicated functions you might be using for your model need gradients for back-propagation. Autograd does this auto-magically! (Sorry, you still need to do this by hand for homework 4.)\n",
    "\n",
    "Let's warmup.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# A tensor that will remember gradients\n",
    "x = torch.randn(1, requires_grad=True)\n",
    "print(x)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-02-20T16:35:28.581665Z",
     "start_time": "2019-02-20T16:35:28.577506Z"
    }
   },
   "source": [
    "At first the 'grad' parameter is None:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-02-20T16:35:28.585548Z",
     "start_time": "2019-02-20T16:35:28.583493Z"
    }
   },
   "outputs": [],
   "source": [
    "print(x.grad)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's do an operation. Take y = e^x.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-02-20T16:35:28.591506Z",
     "start_time": "2019-02-20T16:35:28.587586Z"
    }
   },
   "outputs": [],
   "source": [
    "y = x.exp()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-02-20T16:35:28.596276Z",
     "start_time": "2019-02-20T16:35:28.593558Z"
    }
   },
   "source": [
    "To run the gradient computing magic, call '.backward()' on a variable.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-02-20T16:35:28.602746Z",
     "start_time": "2019-02-20T16:35:28.598538Z"
    }
   },
   "outputs": [],
   "source": [
    "y.backward()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-02-20T16:35:28.610474Z",
     "start_time": "2019-02-20T16:35:28.605443Z"
    }
   },
   "source": [
    "For all dependent variables {x_1, ..., x_n} that were used to compute y, dy/x_i is computed and stored in the x_i.grad field.\n",
    "\n",
    "Here dy/dx = e^x = y. Let's see!\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-02-20T16:35:28.617215Z",
     "start_time": "2019-02-20T16:35:28.613422Z"
    }
   },
   "outputs": [],
   "source": [
    "print(x.grad, y)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Important!** Remember to zero gradients before subsequent calls to backwards.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compute another thingy with x.\n",
    "z = x * 2\n",
    "z.backward()\n",
    "\n",
    "# Should be 2! But it will be 2 + e^x.\n",
    "print(x.grad)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_a = torch.randn(1, requires_grad=True)\n",
    "x_b = torch.randn(1, requires_grad=True)\n",
    "x = x_a * x_b\n",
    "x1 = x**2\n",
    "x2 = 1 / x1\n",
    "x3 = x2.exp()\n",
    "x4 = 1 + x3\n",
    "x5 = x4.log()\n",
    "x6 = x5 ** (1 / 3)\n",
    "x6.backward()\n",
    "print(x_a.grad)\n",
    "print(x_b.grad)\n",
    "\n",
    "\n",
    "x = torch.randn(1, requires_grad=True)\n",
    "y = torch.tanh(x)\n",
    "y.backward()\n",
    "print(x.grad)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Also important!** Under the hood PyTorch stores all the stuff required to compute gradients (call stack, cached values, etc). If you want to save a variable just to keep it around (say for logging or plotting) remember to call `.item()` to get the python value and free the PyTorch machinery memory.\n",
    "\n",
    "You can stop auto-grad from running in the background by using the `torch.no_grad()` context manager.\n",
    "\n",
    "```python\n",
    "with torch.no_grad():\n",
    "    do_all_my_things()\n",
    "```\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-02-20T16:35:28.624048Z",
     "start_time": "2019-02-20T16:35:28.619908Z"
    }
   },
   "source": [
    "## Manual Neural Net + Autograd SGD Example (read this while studying unit 3)\n",
    "\n",
    "Before we move on to the full PyTorch wrapper library, let's do a simple NN SGD example by hand.\n",
    "\n",
    "We'll train a one hidden layer feed forward NN on a toy dataset.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set our random seeds\n",
    "import random\n",
    "import numpy as np\n",
    "\n",
    "\n",
    "def set_seed(seed):\n",
    "    random.seed(seed)\n",
    "    np.random.seed(seed)\n",
    "    torch.manual_seed(seed)\n",
    "    if torch.cuda.is_available():\n",
    "        torch.cuda.manual_seed(seed)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-02-20T16:35:28.630303Z",
     "start_time": "2019-02-20T16:35:28.626019Z"
    }
   },
   "outputs": [],
   "source": [
    "# Get ourselves a simple dataset\n",
    "from sklearn.datasets import make_classification\n",
    "set_seed(7)\n",
    "X, Y = make_classification(n_features=2, n_redundant=0, n_informative=1, n_clusters_per_class=1)\n",
    "print('Number of examples: %d' % X.shape[0])\n",
    "print('Number of features: %d' % X.shape[1])\n",
    "\n",
    "# Take a peak\n",
    "%matplotlib inline\n",
    "import matplotlib\n",
    "import matplotlib.pyplot as plt\n",
    "plt.scatter(X[:, 0], X[:, 1], marker='o', c=Y, s=25, edgecolor='k')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Convert data to PyTorch\n",
    "X, Y = torch.from_numpy(X), torch.from_numpy(Y)\n",
    "\n",
    "# Gotcha: \"Expected object of scalar type Float but got scalar type Double\"\n",
    "# If you see this it's because numpy defaults to Doubles whereas pytorch has floats.\n",
    "X, Y = X.float(), Y.float()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We'll train a one layer neural net to classify this dataset. Let's define the parameter sizes:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-02-19T17:16:34.417254Z",
     "start_time": "2019-02-19T17:16:34.411064Z"
    },
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Define dimensions\n",
    "num_feats = 2\n",
    "hidden_size = 100\n",
    "num_outputs = 1\n",
    "\n",
    "# Learning rate\n",
    "eta = 0.1\n",
    "num_steps = 1000"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "And now run a few steps of SGD!\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-02-20T16:35:28.634813Z",
     "start_time": "2019-02-20T16:35:28.632236Z"
    }
   },
   "outputs": [],
   "source": [
    "# Input to hidden weights\n",
    "W1 = torch.randn(hidden_size, num_feats, requires_grad=True)\n",
    "b1 = torch.zeros(hidden_size, requires_grad=True)\n",
    "\n",
    "# Hidden to output\n",
    "W2 = torch.randn(num_outputs, hidden_size, requires_grad=True)\n",
    "b2 = torch.zeros(num_outputs, requires_grad=True)\n",
    "\n",
    "# Group parameters\n",
    "parameters = [W1, b1, W2, b2]\n",
    "\n",
    "# Get random order\n",
    "indices = torch.randperm(X.size(0))\n",
    "\n",
    "# Keep running average losses for a learning curve?\n",
    "avg_loss = []\n",
    "\n",
    "# Run!\n",
    "for step in range(num_steps):\n",
    "    # Get example\n",
    "    i = indices[step % indices.size(0)]\n",
    "    x_i, y_i = X[i], Y[i]\n",
    "\n",
    "    # Run example\n",
    "    hidden = torch.relu(W1.matmul(x_i) + b1)\n",
    "    y_hat = torch.sigmoid(W2.matmul(hidden) + b2)\n",
    "\n",
    "    # Compute loss binary cross entropy: -(y_i * log(y_hat) + (1 - y_i) * log(1 - y_hat))\n",
    "    # Epsilon for numerical stability\n",
    "    eps = 1e-6\n",
    "    loss = -(y_i * (y_hat + eps).log() + (1 - y_i) * (1 - y_hat + eps).log())\n",
    "\n",
    "    # Add to our running average learning curve. Don't forget .item()!\n",
    "    if step == 0:\n",
    "        avg_loss.append(loss.item())\n",
    "    else:\n",
    "        old_avg = avg_loss[-1]\n",
    "        new_avg = (loss.item() + old_avg * len(avg_loss)) / (len(avg_loss) + 1)\n",
    "        avg_loss.append(new_avg)\n",
    "\n",
    "    # Zero out all previous gradients\n",
    "    for param in parameters:\n",
    "        # It might start out as None\n",
    "        if param.grad is not None:\n",
    "            # In place\n",
    "            param.grad.zero_()\n",
    "\n",
    "    # Backward pass\n",
    "    loss.backward()\n",
    "\n",
    "    # Update parameters\n",
    "    for param in parameters:\n",
    "        # In place!\n",
    "        param.data = param.data - eta * param.grad\n",
    "\n",
    "\n",
    "plt.plot(range(num_steps), avg_loss)\n",
    "plt.ylabel(\"Loss\")\n",
    "plt.xlabel(\"Step\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-02-20T16:35:28.639328Z",
     "start_time": "2019-02-20T16:35:28.636394Z"
    }
   },
   "source": [
    "## torch.nn\n",
    "\n",
    "The `nn` package is where all of the cool neural network stuff is. Layers, loss functions, etc.\n",
    "\n",
    "Let's dive in.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-02-20T16:35:28.643927Z",
     "start_time": "2019-02-20T16:35:28.640936Z"
    }
   },
   "source": [
    "### Layers\n",
    "\n",
    "Before we manually defined our linear layers. PyTorch has them for you as sub-classes of `nn.Module`.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.nn as nn\n",
    "\n",
    "# Linear layer: in_features, out_features\n",
    "linear = nn.Linear(10, 10)\n",
    "print(linear)\n",
    "\n",
    "# Convolution layer: in_channels, out_channels, kernel_size, stride\n",
    "conv = nn.Conv2d(1, 20, 5, 1)\n",
    "print(conv)\n",
    "\n",
    "# RNN: num_inputs, num_hidden, num_layers\n",
    "rnn = nn.RNN(10, 10, 1)\n",
    "print(rnn)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(linear.weight)\n",
    "print([k for k, v in conv.named_parameters()])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-02-20T16:35:28.850451Z",
     "start_time": "2019-02-20T16:35:28.645580Z"
    },
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Make our own model!\n",
    "\n",
    "\n",
    "class Net(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(Net, self).__init__()\n",
    "        # 1 input channel to 20 feature maps of 5x5 kernel. Stride 1.\n",
    "        self.conv1 = nn.Conv2d(1, 20, 5, 1)\n",
    "\n",
    "        # 20 input channels to 50 feature maps of 5x5 kernel. Stride 1.\n",
    "        self.conv2 = nn.Conv2d(20, 50, 5, 1)\n",
    "\n",
    "        # Full connected of final 4x4 image to 500 features\n",
    "        self.fc1 = nn.Linear(4 * 4 * 50, 500)\n",
    "\n",
    "        # From 500 to 10 classes\n",
    "        self.fc2 = nn.Linear(500, 10)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = F.relu(self.conv1(x))\n",
    "        x = F.max_pool2d(x, 2, 2)\n",
    "        x = F.relu(self.conv2(x))\n",
    "        x = F.max_pool2d(x, 2, 2)\n",
    "        x = x.view(-1, 4 * 4 * 50)\n",
    "        x = F.relu(self.fc1(x))\n",
    "        x = self.fc2(x)\n",
    "        return F.log_softmax(x, dim=1)\n",
    "\n",
    "\n",
    "# Initialize it\n",
    "model = Net()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A note on convolution sizes:\n",
    "\n",
    "Running a kernel over the image reduces the image height/length by kernel_size - 1.\n",
    "\n",
    "Running a max pooling over the image reduces the image heigh/length by a factor of the kernel size.\n",
    "\n",
    "So starting from a 28 x 28 image:\n",
    "\n",
    "- Run 5x5 conv --> 24 x 24\n",
    "- Apply 2x2 max pool --> 12 x 12\n",
    "- Run 5x5 conv --> 8 x 8\n",
    "- Apply 2x2 max pool --> 4 x 4\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Optimizers\n",
    "\n",
    "PyTorch handles all the optimizing too. There are several algorithms you can learn about later. Here's SGD:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import torch.optim as optim\n",
    "\n",
    "# Initialize with model parameters\n",
    "optimizer = optim.SGD(model.parameters(), lr=0.01)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Updating is now as easy as:\n",
    "\n",
    "```python\n",
    "loss = loss_fn()\n",
    "optimizer.zero_grad()\n",
    "loss.backward()\n",
    "optimizer.step()\n",
    "```\n",
    "\n",
    "### Full train and test loops\n",
    "\n",
    "Let's look at a full train loop now.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import tqdm\n",
    "import torch.nn.functional as F\n",
    "\n",
    "\n",
    "def train(model, train_loader, optimizer, epoch):\n",
    "    # For things like dropout\n",
    "    model.train()\n",
    "\n",
    "    # Avg loss\n",
    "    total_loss = 0\n",
    "\n",
    "    # Iterate through dataset\n",
    "    for data, target in tqdm.tqdm(train_loader):\n",
    "        # Zero grad\n",
    "        optimizer.zero_grad()\n",
    "\n",
    "        # Forward pass\n",
    "        output = model(data)\n",
    "\n",
    "        # Negative log likelihood loss function\n",
    "        loss = F.nll_loss(output, target)\n",
    "\n",
    "        # Backward pass\n",
    "        loss.backward()\n",
    "        total_loss += loss.item()\n",
    "\n",
    "        # Update\n",
    "        optimizer.step()\n",
    "\n",
    "    # Print average loss\n",
    "    print(\n",
    "        \"Train Epoch: {}\\t Loss: {:.6f}\".format(epoch, total_loss / len(train_loader))\n",
    "    )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Testing loops are similar.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def test(model, test_loader):\n",
    "    model.eval()\n",
    "    test_loss = 0\n",
    "    correct = 0\n",
    "    with torch.no_grad():\n",
    "        for data, target in test_loader:\n",
    "            output = model(data)\n",
    "            test_loss += F.nll_loss(\n",
    "                output, target, reduction=\"sum\"\n",
    "            ).item()  # sum up batch loss\n",
    "            pred = output.argmax(\n",
    "                dim=1, keepdim=True\n",
    "            )  # get the index of the max log-probability\n",
    "            correct += pred.eq(target.view_as(pred)).sum().item()\n",
    "\n",
    "    test_loss /= len(test_loader.dataset)\n",
    "\n",
    "    print(\n",
    "        \"\\nTest set: Average loss: {:.4f}, Accuracy: {}/{} ({:.0f}%)\\n\".format(\n",
    "            test_loss,\n",
    "            correct,\n",
    "            len(test_loader.dataset),\n",
    "            100.0 * correct / len(test_loader.dataset),\n",
    "        )\n",
    "    )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## MNIST\n",
    "\n",
    "Just going to run mnist!\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torchvision import datasets, transforms\n",
    "\n",
    "# See the torch DataLoader for more details.\n",
    "train_loader = torch.utils.data.DataLoader(\n",
    "    datasets.MNIST(\n",
    "        \"../data\",\n",
    "        train=True,\n",
    "        download=True,\n",
    "        transform=transforms.Compose(\n",
    "            [transforms.ToTensor(), transforms.Normalize((0.1307,), (0.3081,))]\n",
    "        ),\n",
    "    ),\n",
    "    batch_size=32,\n",
    "    shuffle=True,\n",
    ")\n",
    "\n",
    "test_loader = torch.utils.data.DataLoader(\n",
    "    datasets.MNIST(\n",
    "        \"../data\",\n",
    "        train=False,\n",
    "        transform=transforms.Compose(\n",
    "            [transforms.ToTensor(), transforms.Normalize((0.1307,), (0.3081,))]\n",
    "        ),\n",
    "    ),\n",
    "    batch_size=32,\n",
    "    shuffle=True,\n",
    ")\n",
    "\n",
    "\n",
    "for epoch in range(1, 10 + 1):\n",
    "    train(model, train_loader, optimizer, epoch)\n",
    "    test(model, test_loader)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.5"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {
    "height": "calc(100% - 180px)",
    "left": "10px",
    "top": "150px",
    "width": "164px"
   },
   "toc_section_display": true,
   "toc_window_display": true
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
